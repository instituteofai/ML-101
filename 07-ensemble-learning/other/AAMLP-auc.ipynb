{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"e3e29410-b8aa-41bb-bd5a-799988dd01de"},"source":"import numpy as np\nfrom functools import partial\nfrom scipy.optimize import fmin\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"a1728fcc-e0da-4dd9-a493-8af65e18242e"},"source":"class OptimizeAUC:\n    \"\"\"\n    Class for optimizing AUC.\n    This class is all you need to find best weights for\n    any model and for any metric and for any types of predictions.\n    With very small changes, this class can be used for optimization of\n    weights in ensemble models of _any_ type of predictions\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n    def _auc(self, coef, X, y):\n        \"\"\"\n        This functions calulates and returns AUC.\n        :param coef: coef list, of the same length as number of models\n        :param X: predictions, in this case a 2d array\n        :param y: targets, in our case binary 1d array\n        \"\"\"\n        # multiply coefficients with every column of the array\n        # with predictions.\n        # this means: element 1 of coef is multiplied by column 1\n        # of the prediction array, element 2 of coef is multiplied\n        # by column 2 of the prediction array and so on!\n        x_coef = X * coef\n        # create predictions by taking row wise sum\n        predictions = np.sum(x_coef, axis=1)\n        # calculate auc score\n        auc_score = metrics.roc_auc_score(y, predictions)\n        # return negative auc\n        return -1.0 * auc_score\n    def fit(self, X, y):\n        # remember partial from hyperparameter optimization chapter?\n        loss_partial = partial(self._auc, X=X, y=y)\n        # dirichlet distribution. you can use any distribution you want\n        # to initialize the coefficients\n        # we want the coefficients to sum to 1\n        initial_coef = np.random.dirichlet(np.ones(X.shape[1]), size=1)\n        # use scipy fmin to minimize the loss function, in our case auc\n        self.coef_ = fmin(loss_partial, initial_coef, disp=True)\n    def predict(self, X):\n        # this is similar to _auc function\n        x_coef = X * self.coef_\n        predictions = np.sum(x_coef, axis=1)\n        return predictions","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"c3278c59-e993-4153-9f9a-b34fe6cd8605"},"source":"import xgboost as xgb\nfrom sklearn.datasets import make_classification\nfrom sklearn import ensemble\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"381b6fdf-281b-4b18-946b-b699522c014a"},"source":"# make a binary classification dataset with 10k samples\n# and 25 features\nX, y = make_classification(n_samples=10000, n_features=25)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"e810c930-81a8-416a-b32c-baa094b0bdc9"},"source":"print(X[0:2])\nprint(y[0:2])","execution_count":null,"outputs":[{"name":"stdout","text":"[[-1.16114596 -0.06705348  0.68228374  0.25039147  1.83031044 -1.52919373\n  -1.51438313  0.51148971 -0.31872142  0.4559758   2.02752334 -0.55678478\n  -0.11370121  0.480422    2.34226626 -0.91282995  1.43471356 -1.32506662\n  -1.54332998  1.39533247  0.13267095 -0.24780828 -1.1200572   0.02985381\n  -0.89684253]\n [-1.45491571  2.10467117 -3.34367329 -0.42895481  1.68930393 -1.4852722\n   0.41007859  0.31738447 -0.00689742 -1.54539413 -0.13773799 -2.1898403\n  -0.68221999 -0.66493806  0.045197   -0.34000742 -1.05708951  0.99752615\n   0.80062067  0.48148036 -0.86385419 -1.11691812  1.34844624 -0.9420137\n  -0.49791631]]\n[0 0]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"d644e315-c251-470f-9599-d6c31b2d1ac9"},"source":"# split into two folds (for this example)\nxfold1, xfold2, yfold1, yfold2 = model_selection.train_test_split(X, y, test_size=0.5, stratify=y)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"80dd5d1e-c645-43cf-82cd-52d1cd5b0d38"},"source":"# fit models on fold 1 and make predictions on fold 2\n# we have 3 models:\n# logistic regression, random forest and xgboost\nlogreg = linear_model.LogisticRegression()\nrf = ensemble.RandomForestClassifier()\nxgbc = xgb.XGBClassifier()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"b77865a8-9e2d-4088-b4f3-5828902b57fb"},"source":"# fit all models on fold 1 data\nlogreg.fit(xfold1, yfold1)\nrf.fit(xfold1, yfold1)\nxgbc.fit(xfold1, yfold1)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"3fceb4b9-2950-43a6-bd2e-328c2649d899"},"source":"# predict all models on fold 2\n# take probability for class 1\npred_logreg = logreg.predict_proba(xfold2)[:, 1]\npred_rf = rf.predict_proba(xfold2)[:, 1]\npred_xgbc = xgbc.predict_proba(xfold2)[:, 1]","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"43c1ab0b-366f-4bf8-9ea8-fe5f89f1260f"},"source":"# create an average of all predictions\n# that is the simplest ensemble\navg_pred = (pred_logreg + pred_rf + pred_xgbc) / 3\n# a 2d array of all predictions\nfold2_preds = np.column_stack((\n    pred_logreg,\n    pred_rf,\n    pred_xgbc,\n    avg_pred\n))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"e0e1652c-0dc3-4dda-bbf8-8bf9ff309977"},"source":"# calculate and store individual AUC values\naucs_fold2 = []\nfor i in range(fold2_preds.shape[1]):\n    auc = metrics.roc_auc_score(yfold2, fold2_preds[:, i])\n    aucs_fold2.append(auc)\n    \nprint(f\"Fold-2: LR AUC = {aucs_fold2[0]}\")\nprint(f\"Fold-2: RF AUC = {aucs_fold2[1]}\")\nprint(f\"Fold-2: XGB AUC = {aucs_fold2[2]}\")\nprint(f\"Fold-2: Average Pred AUC = {aucs_fold2[3]}\")","execution_count":null,"outputs":[{"name":"stdout","text":"Fold-2: LR AUC = 0.973838729311081\nFold-2: RF AUC = 0.9788341980849811\nFold-2: XGB AUC = 0.9787827577886848\nFold-2: Average Pred AUC = 0.9787222774403179\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"262fb9e8-b104-4017-9b2c-470a8d0391b1"},"source":"# now we repeat the same for the other fold\n# this is not the ideal way, if you ever have to repeat code,\n# create a function!\n# fit models on fold 2 and make predictions on fold 1\nlogreg = linear_model.LogisticRegression()\nrf = ensemble.RandomForestClassifier()\nxgbc = xgb.XGBClassifier()\n\nlogreg.fit(xfold2, yfold2)\nrf.fit(xfold2, yfold2)\nxgbc.fit(xfold2, yfold2)\n\npred_logreg = logreg.predict_proba(xfold1)[:, 1]\npred_rf = rf.predict_proba(xfold1)[:, 1]\npred_xgbc = xgbc.predict_proba(xfold1)[:, 1]\navg_pred = (pred_logreg + pred_rf + pred_xgbc) / 3\n\nfold1_preds = np.column_stack((\n    pred_logreg,\n    pred_rf,\n    pred_xgbc,\n    avg_pred\n))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"b20b7405-3352-4ba8-acbc-37ed6fee6f6b"},"source":"aucs_fold1 = []\nfor i in range(fold1_preds.shape[1]):\n    auc = metrics.roc_auc_score(yfold1, fold1_preds[:, i])\n    aucs_fold1.append(auc)\n\nprint(f\"Fold-1: LR AUC = {aucs_fold1[0]}\")\nprint(f\"Fold-1: RF AUC = {aucs_fold1[1]}\")\nprint(f\"Fold-1: XGB AUC = {aucs_fold1[2]}\")\nprint(f\"Fold-1: Average prediction AUC = {aucs_fold1[3]}\")\n\n\n# find optimal weights using the optimizer\nopt = OptimizeAUC()\n# dont forget to remove the average column\nopt.fit(fold1_preds[:, :-1], yfold1)\nopt_preds_fold2 = opt.predict(fold2_preds[:, :-1])\nauc = metrics.roc_auc_score(yfold2, opt_preds_fold2)\nprint(f\"Optimized AUC, Fold 2 = {auc}\")\nprint(f\"Coefficients = {opt.coef_}\")\n\nopt = OptimizeAUC()\nopt.fit(fold2_preds[:, :-1], yfold2)\nopt_preds_fold1 = opt.predict(fold1_preds[:, :-1])\nauc = metrics.roc_auc_score(yfold1, opt_preds_fold1)\nprint(f\"Optimized AUC, Fold 1 = {auc}\")\nprint(f\"Coefficients = {opt.coef_}\")","execution_count":null,"outputs":[{"name":"stdout","text":"Fold-1: LR AUC = 0.9749918039357429\nFold-1: RF AUC = 0.9809778508663507\nFold-1: XGB AUC = 0.9792364772139815\nFold-1: Average prediction AUC = 0.9807358489690559\nOptimization terminated successfully.\n         Current function value: -0.981298\n         Iterations: 31\n         Function evaluations: 71\nOptimized AUC, Fold 2 = 0.9789568387913914\nCoefficients = [0.16236079 0.68541644 0.15476631]\nOptimization terminated successfully.\n         Current function value: -0.979116\n         Iterations: 56\n         Function evaluations: 120\nOptimized AUC, Fold 1 = 0.9812297728414191\nCoefficients = [5.01177824e-06 7.39750718e-01 2.86136316e-02]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We see that average is better but using the optimizer to find the threshold is even\nbetter! Sometimes, the average is the best choice. As you can see, the coefficients\ndo not add up to 1.0, but thatâ€™s okay as we are dealing with AUC and AUC cares\nonly about ranks.","metadata":{"tags":[],"cell_id":"5fe506a5-00a6-481c-a60e-2eed17783751"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"95cfa686-1a41-42f0-ae34-0611b140dc01","deepnote_execution_queue":[]}}